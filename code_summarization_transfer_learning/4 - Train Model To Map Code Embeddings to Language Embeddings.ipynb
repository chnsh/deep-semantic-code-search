{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "You should have completed steps 1-3 of this tutorial before beginning this exercise.  The files required for this notebook are generated by those previous steps.\n",
    "\n",
    "This notebook takes approximately 3 hours to run on an AWS `p3.8xlarge` instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: you can set what GPU you want to use in a notebook like this.  \n",
    "# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from seq2seq_utils import extract_encoder_model, load_encoder_inputs\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from seq2seq_utils import load_text_processor\n",
    "\n",
    "#where you will save artifacts from this step\n",
    "OUTPUT_PATH = Path('./data/code2emb/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# These are where the artifacts are stored from steps 2 and 3, respectively.\n",
    "seq2seq_path = Path('./data/seq2seq/')\n",
    "langemb_path = Path('./data/lang_model_emb/')\n",
    "\n",
    "# set seeds\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model That Maps Code To Sentence Embedding Space\n",
    "\n",
    "In step 2, we trained a seq2seq model that can summarize function code using `(code, docstring)` pairs as the training data.  \n",
    "\n",
    "In this step, we will fine tune the encoder from the seq2seq model to generate code embeddings in the docstring space by using `(code, docstring-embeddings)` as the training data.  Therefore, this notebook will go through the following steps:\n",
    "\n",
    "1. Load the seq2seq model and extract the encoder (remember seq2seq models have an encoder and a decoder).\n",
    "2. Freeze the weights of the encoder.\n",
    "3. Add some dense layers on top of the encoder.\n",
    "4. Train this new model supplying by supplying `(code, docstring-embeddings)` pairs.  We will call this model `code2emb_model`.\n",
    "5. Unfreeze the entire model, and resume training.  This helps fine tune the model a little more towards this task.\n",
    "6. Encode all of the code, including code that does not contain a docstring and save that into a search index for future use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load seq2seq model from Step 2 and extract the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the seq2seq model from Step2, then extract the encoder (we do not need the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1222657, 55)\n"
     ]
    }
   ],
   "source": [
    "# load the pre-processed data for the encoder (we don't care about the decoder in this step)\n",
    "tokens_encoder_input_data, tokens_doc_length = load_encoder_inputs(seq2seq_path/'train.tokens.npy')\n",
    "tokens_seq2seq_Model = load_model(seq2seq_path/'code_summary_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1222657, 45)\n"
     ]
    }
   ],
   "source": [
    "# load the pre-processed data for the encoder (we don't care about the decoder in this step)\n",
    "apiseq_encoder_input_data, apiseq_doc_length = load_encoder_inputs(seq2seq_path/'train.apiseq.npy')\n",
    "apiseq_seq2seq_Model = load_model(seq2seq_path/'api_seq_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1222657, 5)\n"
     ]
    }
   ],
   "source": [
    "# load the pre-processed data for the encoder (we don't care about the decoder in this step)\n",
    "methname_encoder_input_data, methname_doc_length = load_encoder_inputs(seq2seq_path/'train.methname.npy')\n",
    "methname_seq2seq_Model = load_model(seq2seq_path/'methname_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "Body-Word-Embedding (Embeddi (None, 55, 800)           8001600   \n",
      "_________________________________________________________________\n",
      "Encoder-Batchnorm-1 (BatchNo (None, 55, 800)           3200      \n",
      "_________________________________________________________________\n",
      "Encoder-Last-GRU (GRU)       [(None, 1000), (None, 100 5403000   \n",
      "=================================================================\n",
      "Total params: 13,407,800\n",
      "Trainable params: 0\n",
      "Non-trainable params: 13,407,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Extract Encoder from seq2seq model\n",
    "token_encoder_model = extract_encoder_model(tokens_seq2seq_Model)\n",
    "# Get a summary of the encoder and its layers\n",
    "token_encoder_model.name = 'Token-Encoder-Model'\n",
    "token_encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "Body-Word-Embedding (Embeddi (None, 45, 800)           8001600   \n",
      "_________________________________________________________________\n",
      "Encoder-Batchnorm-1 (BatchNo (None, 45, 800)           3200      \n",
      "_________________________________________________________________\n",
      "Encoder-Last-GRU (GRU)       [(None, 1000), (None, 100 5403000   \n",
      "=================================================================\n",
      "Total params: 13,407,800\n",
      "Trainable params: 0\n",
      "Non-trainable params: 13,407,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Extract Encoder from seq2seq model\n",
    "apiseq_encoder_model = extract_encoder_model(apiseq_seq2seq_Model)\n",
    "# Get a summary of the encoder and its layers\n",
    "apiseq_encoder_model.name = 'ApiSeq-Encoder-Model'\n",
    "apiseq_encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "Body-Word-Embedding (Embeddi (None, 5, 800)            8001600   \n",
      "_________________________________________________________________\n",
      "Encoder-Batchnorm-1 (BatchNo (None, 5, 800)            3200      \n",
      "_________________________________________________________________\n",
      "Encoder-Last-GRU (GRU)       [(None, 1000), (None, 100 5403000   \n",
      "=================================================================\n",
      "Total params: 13,407,800\n",
      "Trainable params: 0\n",
      "Non-trainable params: 13,407,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Extract Encoder from seq2seq model\n",
    "methname_encoder_model = extract_encoder_model(methname_seq2seq_Model)\n",
    "# Get a summary of the encoder and its layers\n",
    "methname_encoder_model.name = 'Methname-Encoder-Model'\n",
    "methname_encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f59c4568f98> False\n",
      "<keras.layers.embeddings.Embedding object at 0x7f59c45687b8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f59c5e929b0> False\n",
      "<keras.layers.recurrent.GRU object at 0x7f59c4568eb8> False\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f5ebc9ee7b8> False\n",
      "<keras.layers.embeddings.Embedding object at 0x7f5ebc9eea58> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f59c5ebaac8> False\n",
      "<keras.layers.recurrent.GRU object at 0x7f59c46de198> False\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f5a3c42e4e0> False\n",
      "<keras.layers.embeddings.Embedding object at 0x7f5a3c404cc0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f59c1d317f0> False\n",
      "<keras.layers.recurrent.GRU object at 0x7f5a3c404eb8> False\n"
     ]
    }
   ],
   "source": [
    "# Freeze Encoder Model\n",
    "for encoder_model in [token_encoder_model, apiseq_encoder_model, methname_encoder_model]:\n",
    "    for l in encoder_model.layers:\n",
    "        l.trainable = False\n",
    "        print(l, l.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Docstring Embeddings From From Step 3\n",
    "\n",
    "The target for our `code2emb` model will be docstring-embeddings instead of docstrings.  Therefore, we will use the embeddings for docstrings that we computed in step 3.  For this tutorial, we will use the average over all hidden states, which is saved in the file `avg_emb_dim500_v2.npy`.\n",
    "\n",
    "Note that in our experiments, a concatenation of the average, max, and last hidden state worked better than using the average alone.  However, in the interest of simplicity we demonstrate just using the average hidden state.  We leave it as an exercise to the reader to experiment with other approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222657, 500)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Fitlam Embeddings\n",
    "fastailm_emb = np.load(langemb_path/'avg_emb_dim500_v2.npy')\n",
    "\n",
    "# check that the encoder inputs have the same number of rows as the docstring embeddings\n",
    "assert tokens_encoder_input_data.shape[0] == fastailm_emb.shape[0]\n",
    "assert methname_encoder_input_data.shape[0] == fastailm_emb.shape[0]\n",
    "assert apiseq_encoder_input_data.shape[0] == fastailm_emb.shape[0]\n",
    "\n",
    "fastailm_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1222657"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1222657"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastailm_emb.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct `codeFusion` Model Architecture\n",
    "\n",
    "The `codeFusion` model is the the fusion of the tokens, api sequence, and method name encoders followed by a dense layer - this model should feed into the `code2emb` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "\n",
    "token_input = Input(shape=(tokens_doc_length,), name='Token-Input')\n",
    "apiseq_input = Input(shape=(apiseq_doc_length,), name='API-Input')\n",
    "methname_input = Input(shape=(methname_doc_length,), name='Methname-Input')\n",
    "\n",
    "token_out = token_encoder_model(token_input)\n",
    "apiseq_out = apiseq_encoder_model(apiseq_input)\n",
    "methname_out = methname_encoder_model(methname_input)\n",
    "\n",
    "\n",
    "concatenation_layer = Concatenate(name=\"Concatenate-Token-API-Methname\")\\\n",
    "([token_out, apiseq_out, methname_out])\n",
    "\n",
    "codeFusion_layer = Dense(1000, activation='relu')(concatenation_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct `code2emb` Model Architecture\n",
    "\n",
    "The `code2emb` model is the encoder from the seq2seq model with some dense layers added on top.  The output of the last dense layer of this model needs to match the dimensionality of the docstring embedding, which is 500 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first dense layer with batch norm\n",
    "x = Dense(500, activation='relu')(codeFusion_layer)\n",
    "x = BatchNormalization(name='bn-1')(x)\n",
    "out = Dense(500)(x)\n",
    "code2emb_model = Model([token_input, apiseq_input, methname_input], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Token-Input (InputLayer)        (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "API-Input (InputLayer)          (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Methname-Input (InputLayer)     (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Encoder-Model (Model)     (None, 1000)         13407800    Token-Input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ApiSeq-Encoder-Model (Model)    (None, 1000)         13407800    API-Input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Methname-Encoder-Model (Model)  (None, 1000)         13407800    Methname-Input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Concatenate-Token-API-Methname  (None, 3000)         0           Token-Encoder-Model[14][0]       \n",
      "                                                                 ApiSeq-Encoder-Model[12][0]      \n",
      "                                                                 Methname-Encoder-Model[13][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1000)         3001000     Concatenate-Token-API-Methname[0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 500)          500500      dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn-1 (BatchNormalization)       (None, 500)          2000        dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 500)          250500      bn-1[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 43,977,400\n",
      "Trainable params: 3,753,000\n",
      "Non-trainable params: 40,224,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "code2emb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the `code2emb` Model\n",
    "\n",
    "The model we are training is relatively simple - with two dense layers on top of the pre-trained encoder.  We are leaving the encoder frozen at first, then will unfreeze the encoder in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080630 samples, validate on 147359 samples\n",
      "Epoch 1/15\n",
      "1080630/1080630 [==============================] - 87s 81us/step - loss: -0.6078 - val_loss: -0.7058\n",
      "Epoch 2/15\n",
      "1080630/1080630 [==============================] - 85s 79us/step - loss: -0.7338 - val_loss: -0.7195\n",
      "Epoch 3/15\n",
      "1080630/1080630 [==============================] - 85s 78us/step - loss: -0.7405 - val_loss: -0.7271\n",
      "Epoch 4/15\n",
      "1080630/1080630 [==============================] - 84s 78us/step - loss: -0.7434 - val_loss: -0.7298\n",
      "Epoch 5/15\n",
      "1080630/1080630 [==============================] - 84s 78us/step - loss: -0.7452 - val_loss: -0.7303\n",
      "Epoch 6/15\n",
      "1080630/1080630 [==============================] - 84s 78us/step - loss: -0.7466 - val_loss: -0.7312\n",
      "Epoch 7/15\n",
      "1080630/1080630 [==============================] - 84s 78us/step - loss: -0.7477 - val_loss: -0.7309\n",
      "Epoch 8/15\n",
      "1080630/1080630 [==============================] - 85s 78us/step - loss: -0.7487 - val_loss: -0.7332\n",
      "Epoch 9/15\n",
      "1080630/1080630 [==============================] - 83s 77us/step - loss: -0.7497 - val_loss: -0.7320\n",
      "Epoch 10/15\n",
      "1080630/1080630 [==============================] - 83s 77us/step - loss: -0.7504 - val_loss: -0.7310\n",
      "Epoch 11/15\n",
      "1080630/1080630 [==============================] - 84s 77us/step - loss: -0.7511 - val_loss: -0.7301\n",
      "Epoch 12/15\n",
      "1080630/1080630 [==============================] - 83s 77us/step - loss: -0.7519 - val_loss: -0.7301\n",
      "Epoch 13/15\n",
      "1080630/1080630 [==============================] - 84s 77us/step - loss: -0.7524 - val_loss: -0.7282\n",
      "Epoch 14/15\n",
      "1080630/1080630 [==============================] - 83s 77us/step - loss: -0.7530 - val_loss: -0.7307\n",
      "Epoch 15/15\n",
      "1080630/1080630 [==============================] - 84s 77us/step - loss: -0.7534 - val_loss: -0.7318\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "\n",
    "code2emb_model.compile(optimizer=optimizers.Nadam(lr=0.002), loss='cosine_proximity')\n",
    "script_name_base = 'code2emb_model_'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 20000\n",
    "epochs = 15\n",
    "history = code2emb_model.fit([encoder_input_data], fastailm_emb,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.7453`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze all Layers of Model and Resume Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step, we left the encoder frozen.  Now that the dense layers are trained, we will unfreeze the entire model and let it train some more.  This will hopefully allow this model to specialize on this task a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7f65fdb2a940> True\n",
      "<keras.engine.training.Model object at 0x7f65fdb2afd0> True\n",
      "<keras.layers.core.Dense object at 0x7f65fdb2a4e0> True\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f65fdb2a978> True\n",
      "<keras.layers.core.Dense object at 0x7f65fdb2ae10> True\n"
     ]
    }
   ],
   "source": [
    "for l in code2emb_model.layers:\n",
    "    l.trainable = True\n",
    "    print(l, l.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080630 samples, validate on 147359 samples\n",
      "Epoch 17/20\n",
      "1080630/1080630 [==============================] - 93s 86us/step - loss: -0.7541 - val_loss: -0.7430\n",
      "Epoch 18/20\n",
      "1080630/1080630 [==============================] - 93s 86us/step - loss: -0.7544 - val_loss: -0.7431\n",
      "Epoch 19/20\n",
      "1080630/1080630 [==============================] - 93s 86us/step - loss: -0.7546 - val_loss: -0.7432\n",
      "Epoch 20/20\n",
      "1080630/1080630 [==============================] - 93s 86us/step - loss: -0.7548 - val_loss: -0.7431\n"
     ]
    }
   ],
   "source": [
    "code2emb_model.compile(optimizer=optimizers.Nadam(lr=0.0001), loss='cosine_proximity')\n",
    "script_name_base = 'code2emb_model_unfreeze_'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 2000\n",
    "epochs = 20\n",
    "history = code2emb_model.fit([encoder_input_data], fastailm_emb,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          initial_epoch=16,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `code2emb` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2emb_model.save(OUTPUT_PATH/'code2emb_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has been cached and is also available for download here:\n",
    "\n",
    "`code2emb_model.hdf5`:https://storage.googleapis.com/kubeflow-examples/code_search/data/code2emb/code2emb_model.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize all of the code without docstrings\n",
    "\n",
    "We want to vectorize all of the code without docstrings so we can test the efficacy of the search on the code that was never seen by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from seq2seq_utils import load_text_processor\n",
    "code2emb_path = Path('./data/code2emb/')\n",
    "seq2seq_path = Path('./data/seq2seq/')\n",
    "data_path = Path('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n"
     ]
    }
   ],
   "source": [
    "code2emb_model = load_model(code2emb_path/'code2emb_model.hdf5')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(seq2seq_path/'py_code_proc_v2.dpkl')\n",
    "\n",
    "with open(data_path/'without_docstrings.function', 'r') as f:\n",
    "    no_docstring_funcs = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process code without docstrings for input into `code2emb` model\n",
    "\n",
    "We use the same transformer we used to train the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def __init__ self leafs edges self edges edges self leafs sorted leafs\\n',\n",
       " 'def __eq__ self other if isinstance other Node return id self id other or self leafs other leafs and self edges other edges else return False\\n',\n",
       " 'def __repr__ self return Node leafs edges format self leafs self edges\\n',\n",
       " 'staticmethod def _isCapitalized token return len token 1 and token isalpha and token 0 isupper and token 1 islower\\n',\n",
       " 'staticmethod def _isCapitalizeD last token return last and len token 1 and last isalpha and token isupper\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized functions that did not contain docstrigns\n",
    "no_docstring_funcs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n"
     ]
    }
   ],
   "source": [
    "encinp = enc_pp.transform_parallel(no_docstring_funcs)\n",
    "np.save(code2emb_path/'nodoc_encinp.npy', encinp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract code vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "code2emb_path = Path('./data/code2emb/')\n",
    "encinp = np.load(code2emb_path/'nodoc_encinp.npy')\n",
    "code2emb_model = load_model(code2emb_path/'code2emb_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `code2emb` model to map the code into the same vector space as natural language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodoc_vecs = code2emb_model.predict(encinp, batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the number of output rows equal the number of input rows\n",
    "assert nodoc_vecs.shape[0] == encinp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectorized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(code2emb_path/'nodoc_vecs.npy', nodoc_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

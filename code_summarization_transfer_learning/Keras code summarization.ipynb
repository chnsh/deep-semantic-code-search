{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# EN \u003d spacy.load(\u0027en_core_web_sm\u0027)\n",
        "import en_core_web_sm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from general_utils import apply_parallel, flattenlist\n",
        "EN \u003d en_core_web_sm.load()\n",
        "\n",
        "from ktext.preprocess import processor\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "use_cache \u003d True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "df \u003d pd.read_pickle(\u0027./data/dataframe_processed.pkl\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Separate function w/o docstrings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def listlen(x):\n",
        "    if not isinstance(x, list):\n",
        "        return 0\n",
        "    return len(x)\n",
        "\n",
        "# separate functions w/o docstrings\n",
        "# docstrings should be at least 3 words in the docstring to be considered a valid docstring\n",
        "\n",
        "with_docstrings \u003d df[df.docstring_tokens.str.split().apply(listlen) \u003e\u003d 3]\n",
        "without_docstrings \u003d df[df.docstring_tokens.str.split().apply(listlen) \u003c 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Partition code by repository to minimize leakage between train, valid \u0026 test sets. \n",
        "Rough assumption that each repository has its own style.  We want to avoid having code from the same repository in the training set as well as the validation or holdout set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "grouped \u003d with_docstrings.groupby(\u0027nwo\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# train, valid, test splits\n",
        "train, test \u003d train_test_split(list(grouped), train_size\u003d0.87, shuffle\u003dTrue, random_state\u003d8081)\n",
        "# train, valid \u003d train_test_split(train, train_size\u003d0.82, random_state\u003d8081)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train \u003d pd.concat([d for _, d in train]).reset_index(drop\u003dTrue)\n",
        "# valid \u003d pd.concat([d for _, d in valid]).reset_index(drop\u003dTrue)\n",
        "test \u003d pd.concat([d for _, d in test]).reset_index(drop\u003dTrue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(f\u0027train set num rows {train.shape[0]:,}\u0027)\n",
        "# print(f\u0027valid set num rows {valid.shape[0]:,}\u0027)\n",
        "print(f\u0027test set num rows {test.shape[0]:,}\u0027)\n",
        "print(f\u0027without docstring rows {without_docstrings.shape[0]:,}\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Preview what the training set looks like.  You can start to see how the data looks, the function tokens and docstring tokens are what will be fed downstream into the models.  The other information is important for diagnostics and bookeeping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train[\u0027api_sequence\u0027].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "if use_cache:\n",
        "    OUTPUT_PATH \u003d Path(\u0027./data/seq2seq/processors/\u0027)\n",
        "    import dill as dpickle\n",
        "    import numpy as np\n",
        "\n",
        "    with open(OUTPUT_PATH/\u0027function_token_processor.dpkl\u0027, \u0027rb\u0027) as f:\n",
        "        function_token_processor \u003d dpickle.load(f)\n",
        "\n",
        "    with open(OUTPUT_PATH/\u0027docstring_processor.dpkl\u0027, \u0027rb\u0027) as f:\n",
        "        docstring_processor \u003d dpickle.load(f)\n",
        "\n",
        "    with open(OUTPUT_PATH/\u0027methname_processor.dpkl\u0027, \u0027rb\u0027) as f:\n",
        "        methname_processor \u003d dpickle.load(f)\n",
        "\n",
        "    with open(OUTPUT_PATH/\u0027api_seq_processor.dpkl\u0027, \u0027rb\u0027) as f:\n",
        "        api_seq_processor \u003d dpickle.load(f)\n",
        "    \n",
        "    train_token_v \u003d np.load(\u0027./data/seq2seq/train.tokens.npy\u0027)\n",
        "    train_api_seq_v \u003d np.load(\u0027./data/seq2seq/train.apiseq.npy\u0027)\n",
        "    train_methname_v \u003d np.load(\u0027./data/seq2seq/train.methname.npy\u0027)\n",
        "    train_docstring_v \u003d np.load(\u0027./data/seq2seq/train.desc.npy\u0027)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Vectorize the training set by creating a bag of words model with vocabulary size 20,000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "if not use_cache:\n",
        "    from ktext.preprocess import processor\n",
        "    keep_n \u003d 20000\n",
        "\n",
        "    # todo, probably tokens should also be seq to seq\n",
        "    function_token_processor \u003d processor(hueristic_pct_padding\u003d0.7, keep_n\u003dkeep_n, padding\u003d\u0027post\u0027, truncating\u003d\u0027post\u0027)\n",
        "    train_token_v \u003d function_token_processor.fit_transform(train[\u0027function_tokens\u0027])\n",
        "\n",
        "    docstring_processor \u003d processor(append_indicators\u003dTrue, hueristic_pct_padding\u003d0.7, keep_n\u003dkeep_n, padding\u003d\u0027post\u0027, truncating\u003d\u0027post\u0027)\n",
        "    train_docstring_v \u003d docstring_processor.fit_transform(train[\u0027docstring_tokens\u0027])\n",
        "\n",
        "    methname_processor \u003d processor(append_indicators\u003dTrue, hueristic_pct_padding\u003d0.7, keep_n\u003dkeep_n, padding\u003d\u0027post\u0027, truncating\u003d\u0027post\u0027)\n",
        "    train_methname_v \u003d methname_processor.fit_transform(train[\u0027tokenized_function_name\u0027])\n",
        "\n",
        "    api_seq_processor \u003d processor(append_indicators\u003dTrue, hueristic_pct_padding\u003d0.7, keep_n\u003dkeep_n, padding\u003d\u0027post\u0027, truncating\u003d\u0027post\u0027)\n",
        "    train_api_seq_v \u003d api_seq_processor.fit_transform(train[\u0027api_sequence\u0027])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "function_token_processor.document_length_stats.to_csv(\u0027function_token_processor.hist.csv\u0027, index\u003dFalse)\n",
        "docstring_processor.document_length_stats.to_csv(\u0027docstring_processor.hist.csv\u0027, index\u003dFalse)\n",
        "methname_processor.document_length_stats.to_csv(\u0027methname_processor.hist.csv\u0027, index\u003dFalse)\n",
        "api_seq_processor.document_length_stats.to_csv(\u0027api_seq_processor.hist.csv\u0027, index\u003dFalse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(train_token_v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "train_token_v[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(train_docstring_v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(train_api_seq_v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(train_methname_v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "OUTPUT_PATH \u003d Path(\u0027./data/seq2seq/\u0027)\nimport dill as dpickle\nimport numpy as np\n\n\n# Save the preprocessor\nwith open(OUTPUT_PATH/\u0027function_token_processor.dpkl\u0027, \u0027wb\u0027) as f:\n    dpickle.dump(function_token_processor, f)\n\nwith open(OUTPUT_PATH/\u0027docstring_processor.dpkl\u0027, \u0027wb\u0027) as f:\n    dpickle.dump(docstring_processor, f)\n\nwith open(OUTPUT_PATH/\u0027methname_processor.dpkl\u0027, \u0027wb\u0027) as f:\n    dpickle.dump(methname_processor, f)\n\nwith open(OUTPUT_PATH/\u0027api_seq_processor.dpkl\u0027, \u0027wb\u0027) as f:\n    dpickle.dump(api_seq_processor, f)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import tables\n",
        "\n",
        "def save_vecs(vecs, fout):\n",
        "    np.save(fout, vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "save_vecs(train_token_v, \u0027./data/seq2seq/train.tokens.npy\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "save_vecs(train_api_seq_v, \u0027./data/seq2seq/train.apiseq.npy\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "save_vecs(train_methname_v, \u0027./data/seq2seq/train.methname.npy\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "save_vecs(train_docstring_v, \u0027./data/seq2seq/train.desc.npy\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "w_tokens \u003d function_token_processor.transform_parallel(without_docstrings[\u0027function_tokens\u0027])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "w_apiseq \u003d api_seq_processor.transform_parallel(without_docstrings[\u0027api_sequence\u0027])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "w_methname \u003d methname_processor.transform_parallel(without_docstrings[\u0027tokenized_function_name\u0027])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "save_vecs(w_tokens, \u0027./data/seq2seq/without_docstring.tokens.npy\u0027)\n",
        "save_vecs(w_apiseq, \u0027./data/seq2seq/without_docstring.apiseq.npy\u0027)\n",
        "save_vecs(w_methname, \u0027./data/seq2seq/without_docstring.methname.npy\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Generating Test vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "test_token_v \u003d function_token_processor.transform_parallel(test[\u0027function_tokens\u0027])\n",
        "test_api_seq_v \u003d api_seq_processor.transform_parallel(test[\u0027api_sequence\u0027])\n",
        "test_methname_v \u003d methname_processor.transform_parallel(test[\u0027tokenized_function_name\u0027])\n",
        "test_docstring_v \u003d docstring_processor.transform_parallel(test[\u0027docstring_tokens\u0027])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "save_vecs(test_token_v, \u0027./data/seq2seq/test.tokens.npy\u0027)\n",
        "save_vecs(test_api_seq_v, \u0027./data/seq2seq/test.apiseq.npy\u0027)\n",
        "save_vecs(test_methname_v, \u0027./data/seq2seq/test.methname.npy\u0027)\n",
        "save_vecs(test_docstring_v, \u0027./data/seq2seq/test.desc.npy\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Generating Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\u0027./data/seq2seq/vocab.apiseq.pkl\u0027, \u0027wb\u0027) as f:\n",
        "    pickle.dump(api_seq_processor.token2id, f)\n",
        "\n",
        "with open(\u0027./data/seq2seq/vocab.methname.pkl\u0027, \u0027wb\u0027) as f:\n",
        "    pickle.dump(methname_processor.token2id, f)\n",
        "\n",
        "with open(\u0027./data/seq2seq/vocab.desc.pkl\u0027, \u0027wb\u0027) as f:\n",
        "    pickle.dump(docstring_processor.token2id, f)\n",
        "\n",
        "with open(\u0027./data/seq2seq/vocab.tokens.pkl\u0027, \u0027wb\u0027) as f:\n",
        "    pickle.dump(function_token_processor.token2id, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "OUTPUT_PATH \u003d Path(\u0027./data/seq2seq/\u0027)\n",
        "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
        "\n",
        "\n",
        "encoder_input_data, encoder_seq_len \u003d load_encoder_inputs(\u0027./data/seq2seq/train.tokens.npy\u0027)\n",
        "decoder_input_data, decoder_target_data \u003d load_decoder_inputs(\u0027./data/seq2seq/train.desc.npy\u0027)\n",
        "num_encoder_tokens, enc_pp \u003d load_text_processor(OUTPUT_PATH/\u0027function_token_processor.dpkl\u0027)\n",
        "num_decoder_tokens, dec_pp \u003d load_text_processor(OUTPUT_PATH/\u0027docstring_processor.dpkl\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
        "\n",
        "\n",
        "api_encoder_input_data, api_encoder_seq_len \u003d load_encoder_inputs(\u0027./data/seq2seq/train.apiseq.npy\u0027)\n",
        "decoder_input_data, decoder_target_data \u003d load_decoder_inputs(\u0027./data/seq2seq/train.desc.npy\u0027)\n",
        "api_num_encoder_tokens, api_enc_pp \u003d load_text_processor(OUTPUT_PATH/\u0027api_seq_processor.dpkl\u0027)\n",
        "num_decoder_tokens, dec_pp \u003d load_text_processor(OUTPUT_PATH/\u0027docstring_processor.dpkl\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
        "\n",
        "\n",
        "methname_encoder_input_data, methname_encoder_len \u003d load_encoder_inputs(\u0027./data/seq2seq/train.methname.npy\u0027)\n",
        "decoder_input_data, decoder_target_data \u003d load_decoder_inputs(\u0027./data/seq2seq/train.desc.npy\u0027)\n",
        "methname_num_encoder_tokens, methname_enc_pp \u003d load_text_processor(OUTPUT_PATH/\u0027methname_processor.dpkl\u0027)\n",
        "num_decoder_tokens, dec_pp \u003d load_text_processor(OUTPUT_PATH/\u0027docstring_processor.dpkl\u0027)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Build Seq2Seq Model For Summarizing Code\n",
        "\n",
        "We will build a model to predict the docstring given a function or a method.  While this is a very cool task in itself, this is not the end goal of this exercise.  The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import build_seq2seq_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.  \n",
        "\n",
        "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like [attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf).  We encourage you to try and build different architectures to see what works best for you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "seq2seq_Model \u003d build_seq2seq_model(word_emb_dim\u003d800,\n",
        "                                    hidden_state_dim\u003d1000,\n",
        "                                    encoder_seq_len\u003dencoder_seq_len,\n",
        "                                    num_encoder_tokens\u003dnum_encoder_tokens,\n",
        "                                    num_decoder_tokens\u003dnum_decoder_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "seq2seq_Model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "# !which python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Train Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from keras.models import Model, load_model\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "import numpy as np\n",
        "from keras import optimizers\n",
        "\n",
        "import tensorflow as tf\n",
        "# your code here\n",
        "with tf.device(\u0027/gpu:0\u0027):\n",
        "    seq2seq_Model.compile(optimizer\u003doptimizers.Nadam(lr\u003d0.00005), loss\u003d\u0027sparse_categorical_crossentropy\u0027)\n",
        "\n",
        "    script_name_base \u003d \u0027py_func_sum_v9_\u0027\n",
        "    csv_logger \u003d CSVLogger(\u0027{:}.log\u0027.format(script_name_base))\n",
        "\n",
        "    model_checkpoint \u003d ModelCheckpoint(\u0027{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5\u0027.format(script_name_base),\n",
        "                                       save_best_only\u003dTrue)\n",
        "\n",
        "    batch_size \u003d 1100\n",
        "    epochs \u003d 20\n",
        "    history \u003d seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "              batch_size\u003dbatch_size,\n",
        "              epochs\u003depochs,\n",
        "              validation_split\u003d0.12, callbacks\u003d[csv_logger, model_checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Training Seq To Seq for API sequence extracted from AST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import build_seq2seq_model\n",
        "api_seq_Model \u003d build_seq2seq_model(word_emb_dim\u003d800,\n",
        "                                    hidden_state_dim\u003d1000,\n",
        "                                    encoder_seq_len\u003dapi_encoder_seq_len,\n",
        "                                    num_encoder_tokens\u003dapi_num_encoder_tokens,\n",
        "                                    num_decoder_tokens\u003dnum_decoder_tokens)\n",
        "\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, TensorBoard\n",
        "import numpy as np\n",
        "from keras import optimizers\n",
        "\n",
        "import tensorflow as tf\n",
        "# your code here\n",
        "with tf.device(\u0027/gpu:0\u0027):\n",
        "    api_seq_Model.compile(optimizer\u003doptimizers.Nadam(lr\u003d0.00005), loss\u003d\u0027sparse_categorical_crossentropy\u0027)\n",
        "\n",
        "    script_name_base \u003d \u0027api_seq_model_\u0027\n",
        "    csv_logger \u003d CSVLogger(\u0027{:}.log\u0027.format(script_name_base))\n",
        "\n",
        "    model_checkpoint \u003d ModelCheckpoint(\u0027{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5\u0027.format(script_name_base),\n",
        "                                       save_best_only\u003dTrue)\n",
        "    \n",
        "    tensorboard \u003d TensorBoard()\n",
        "\n",
        "    batch_size \u003d 1100\n",
        "    epochs \u003d 20\n",
        "    history \u003d api_seq_Model.fit([api_encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "              batch_size\u003dbatch_size,\n",
        "              epochs\u003depochs,\n",
        "              validation_split\u003d0.12, callbacks\u003d[csv_logger, model_checkpoint, tensorboard])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Method name trianing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import build_seq2seq_model\n",
        "methname_model \u003d build_seq2seq_model(word_emb_dim\u003d800,\n",
        "                                    hidden_state_dim\u003d1000,\n",
        "                                    encoder_seq_len\u003dmethname_encoder_len,\n",
        "                                    num_encoder_tokens\u003dmethname_num_encoder_tokens,\n",
        "                                    num_decoder_tokens\u003dnum_decoder_tokens)\n",
        "\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "import numpy as np\n",
        "from keras import optimizers\n",
        "\n",
        "import tensorflow as tf\n",
        "# your code here\n",
        "with tf.device(\u0027/gpu:0\u0027):\n",
        "    methname_model.compile(optimizer\u003doptimizers.Nadam(lr\u003d0.00005), loss\u003d\u0027sparse_categorical_crossentropy\u0027)\n",
        "\n",
        "    script_name_base \u003d \u0027methname_model_\u0027\n",
        "    csv_logger \u003d CSVLogger(\u0027{:}.log\u0027.format(script_name_base))\n",
        "\n",
        "    model_checkpoint \u003d ModelCheckpoint(\u0027{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5\u0027.format(script_name_base),\n",
        "                                       save_best_only\u003dTrue)\n",
        "\n",
        "    batch_size \u003d 1100\n",
        "    epochs \u003d 20\n",
        "    history \u003d methname_model.fit([methname_encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "              batch_size\u003dbatch_size,\n",
        "              epochs\u003depochs,\n",
        "              validation_split\u003d0.12, callbacks\u003d[csv_logger, model_checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Manually Inspect Results (on holdout set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "seq2seq_Model \u003d load_model(\u0027py_func_sum_v9_.epoch20-val2.39946.hdf5\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import Seq2Seq_Inference\n",
        "import pandas as pd\n",
        "\n",
        "seq2seq_inf \u003d Seq2Seq_Inference(encoder_preprocessor\u003denc_pp,\n",
        "                                 decoder_preprocessor\u003ddec_pp,\n",
        "                                 seq2seq_model\u003dseq2seq_Model)\n",
        "\n",
        "demo_testdf \u003d pd.DataFrame({\u0027code\u0027:test[\u0027function_tokens\u0027], \u0027comment\u0027:test[\u0027docstring_tokens\u0027], \u0027ref\u0027:\u0027\u0027})\n",
        "seq2seq_inf.demo_model_predictions(n\u003d15, df\u003ddemo_testdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "api_seq_Model \u003d load_model(\u0027api_seq_model_.epoch20-val2.58093.hdf5\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import Seq2Seq_Inference\n",
        "import pandas as pd\n",
        "\n",
        "api_seq_Model_inf \u003d Seq2Seq_Inference(encoder_preprocessor\u003dapi_enc_pp,\n",
        "                                 decoder_preprocessor\u003ddec_pp,\n",
        "                                 seq2seq_model\u003dapi_seq_Model)\n",
        "\n",
        "demo_testdf \u003d pd.DataFrame({\u0027code\u0027:test[\u0027api_sequence\u0027], \u0027comment\u0027:test[\u0027docstring_tokens\u0027], \u0027ref\u0027:\u0027\u0027})\n",
        "api_seq_Model_inf.demo_model_predictions(n\u003d15, df\u003ddemo_testdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "methname_model \u003d load_model(\u0027methname_model_.epoch20-val2.59926.hdf5\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from seq2seq_utils import Seq2Seq_Inference\n",
        "import pandas as pd\n",
        "\n",
        "methname_inf \u003d Seq2Seq_Inference(encoder_preprocessor\u003dmethname_enc_pp,\n",
        "                                 decoder_preprocessor\u003ddec_pp,\n",
        "                                 seq2seq_model\u003dmethname_model)\n",
        "\n",
        "demo_testdf \u003d pd.DataFrame({\u0027code\u0027:test[\u0027tokenized_function_name\u0027], \u0027comment\u0027:test[\u0027docstring_tokens\u0027], \u0027ref\u0027:\u0027\u0027})\n",
        "methname_inf.demo_model_predictions(n\u003d15, df\u003ddemo_testdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Save model to disk\n",
        "\n",
        "Save the model to disk so you can use it in Step 4 of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "seq2seq_Model.save(OUTPUT_PATH/\u0027code_summary_seq2seq_model.h5\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "methname_model.save(OUTPUT_PATH/\u0027methname_seq2seq_model.h5\u0027)\n",
        "api_seq_Model.save(OUTPUT_PATH/\u0027api_seq_seq2seq_model.h5\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}